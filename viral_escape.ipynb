{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/viral-escape/blob/main/viral_escape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JFLN3xhJEK9"
      },
      "source": [
        "\n",
        "## Learning the language of viral escape\n",
        "1.   paper \n",
        "2.   supplementary material\n",
        "3.   code https://github.com/brianhie/viral-mutation/blob/master/bin/language_model.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> used a BiLSTM architecture with two hidden layers of 512\n",
        "hidden units each, with an Adam learning rate of 0.001. We used the same architecture for all\n",
        "experiments. We train the language model to predict the observed amino acid residue at all\n",
        "positions in each sequence, using the remaining sequence as the input; one training epoch is\n",
        "completed when the model has considered all positions in all sequences in the training corpus.\n",
        "We trained each model until convergence of cross entropy loss across one training epoch."
      ],
      "metadata": {
        "id": "81PcNx94X7Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PHlsIIoMxtZ",
        "outputId": "ebe33cf9-5f47-46d8-a51e-a48f249d255e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  3 21:38:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlv-Ga8HM49f",
        "outputId": "40fad321-0d0e-4a02-acf9-63c07a438743"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a new model on flu HA sequences can be done with the command\n"
      ],
      "metadata": {
        "id": "OwiBlsn6dMPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "t2k5TjuBRS_n"
      },
      "outputs": [],
      "source": [
        "#! python bin/flu.py bilstm --train --test > flu_train.log 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4qJxdFCQ6as",
        "outputId": "c70ae3aa-7e54-45bf-e53f-1c5a20f0bc05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'viral-mutation' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# clone repository\n",
        "! git clone https://github.com/brianhie/viral-mutation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C16K5EBTIVOS",
        "outputId": "fec8fcfa-8e0c-40b9-ac9e-fb18f3112dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-03 18:31:37--  http://cb.csail.mit.edu/cb/viral-mutation/data.tar.gz\n",
            "Resolving cb.csail.mit.edu (cb.csail.mit.edu)... 128.30.2.148\n",
            "Connecting to cb.csail.mit.edu (cb.csail.mit.edu)|128.30.2.148|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3060938794 (2.9G) [application/x-gzip]\n",
            "Saving to: ‘data.tar.gz’\n",
            "\n",
            "data.tar.gz         100%[===================>]   2.85G  40.8MB/s    in 73s     \n",
            "\n",
            "2022-01-03 18:32:51 (40.0 MB/s) - ‘data.tar.gz’ saved [3060938794/3060938794]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download data\n",
        "!wget http://cb.csail.mit.edu/cb/viral-mutation/data.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbRtBzySIipJ",
        "outputId": "57ff2f78-bc53-4b63-bb29-ceb9f4a745bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/escape_results.txt\n",
            "data/evcouplings/\n",
            "data/evcouplings/flu_h3_config.yaml\n",
            "data/evcouplings/flu_h1_config.yaml\n",
            "data/evcouplings/hiv_bf520_seq.fa\n",
            "data/evcouplings/hiv_env_seq.fa\n",
            "data/evcouplings/hiv_env_config.yaml\n",
            "data/evcouplings/sarscov2_config.yaml\n",
            "data/evcouplings/flu_h1_seq.fa\n",
            "data/evcouplings/hiv_bf520_config.yaml\n",
            "data/evcouplings/sarscov2_seq.fa\n",
            "data/evcouplings/flu_h3_seq.fa\n",
            "data/headlines/\n",
            "data/headlines/abcnews-date-text.csv\n",
            "data/headlines/headlines.txt\n",
            "data/hiv/\n",
            "data/hiv/bg505_regions.txt\n",
            "data/hiv/fitness_haddox2018/\n",
            "data/hiv/fitness_haddox2018/BG505-1_prefs.csv\n",
            "data/hiv/fitness_haddox2018/BG505-3_prefs.csv\n",
            "data/hiv/fitness_haddox2018/BG505-2_prefs.csv\n",
            "data/hiv/fitness_haddox2018/BF520-3_prefs.csv\n",
            "data/hiv/fitness_haddox2018/BF520-2_prefs.csv\n",
            "data/hiv/fitness_haddox2018/BF520-1_prefs.csv\n",
            "data/hiv/fitness_haddox2018/BF520_to_HXB2.csv\n",
            "data/hiv/fitness_haddox2018/BF520_env.fasta\n",
            "data/hiv/fitness_haddox2018/map_indices.py\n",
            "data/hiv/fitness_haddox2018/BF520_avgprefs.csv\n",
            "data/hiv/fitness_haddox2018/Env_protalignment_manualtweaks.fasta\n",
            "data/hiv/fitness_haddox2018/BG505_env.fasta\n",
            "data/hiv/fitness_haddox2018/BG505_avgprefs.csv\n",
            "data/hiv/fitness_haddox2018/BG505_to_HXB2.csv\n",
            "data/hiv/HIV-1_env_samelen.fa\n",
            "data/hiv/escape_dingens2019/\n",
            "data/hiv/escape_dingens2019/FileS4/\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/VRC34.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/PGT151.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/VRC01.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/10E8.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/3BNC117.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/PGT145.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/PGT121.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/101074.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/PG9.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurvive/3BNC117-101074-pool.csv\n",
            "data/hiv/escape_dingens2019/FileS4/.DS_Store\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/PG9.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/PGT151.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/3BNC117.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/3BNC117-101074-pool.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/PGT121.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/101074.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/VRC34.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/PGT145.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/VRC01.csv\n",
            "data/hiv/escape_dingens2019/FileS4/fracsurviveaboveavg/10E8.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/PGT121.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/101074.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/PGT145.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/10E8.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/.DS_Store\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/PG9.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/VRC34.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/3BNC117.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/3BNC117-101074-pool.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/VRC01.csv\n",
            "data/hiv/escape_dingens2019/FileS4/diffsel/PGT151.csv\n",
            "data/hiv/escape_dingens2019/BG505_to_HXB2.csv\n",
            "data/hiv/escape_dingens2019/Env_protalign_manualeditAD.fasta\n",
            "data/fitness_results.txt\n",
            "data/influenza/\n",
            "data/influenza/escape_lee2019/\n",
            "data/influenza/escape_lee2019/H3_site_to_PDB_4o5n.csv\n",
            "data/influenza/escape_lee2019/avg_sel_tidy.csv\n",
            "data/influenza/escape_lee2019/Perth2009_H3_HA.fa\n",
            "data/influenza/fitness_wu2020/\n",
            "data/influenza/fitness_wu2020/data_all.csv\n",
            "data/influenza/fitness_wu2020/HA_ecto.fa\n",
            "data/influenza/fitness_wu2020/data_pref.tsv\n",
            "data/influenza/fitness_wu2020/HumanH3N2_All_2018.fa\n",
            "data/influenza/fitness_wu2020/wildtypes.fa\n",
            "data/influenza/escape_doud2018/\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_FI6v3_median_avgsite.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_H17L7_median.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_S139_median.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_C179_median_avgsite.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_H17L7_median_avgsite.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_FI6v3_median.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_C179_median.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_H17L10_median_avgsite.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_S139_median_avgsite.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_H17L19_median.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_H17L19_median_avgsite.csv\n",
            "data/influenza/escape_doud2018/medianfracsurvivefiles/antibody_H17L10_median.csv\n",
            "data/influenza/escape_doud2018/pos_map.csv\n",
            "data/influenza/escape_doud2018/H1toH3_renumber.csv\n",
            "data/influenza/escape_doud2018/WSN1933_H1_HA.fa\n",
            "data/influenza/escape_doud2018/candidates.fa\n",
            "data/influenza/ird_influenzaA_HA_allspecies.fa\n",
            "data/influenza/ird_influenzaA_HA_allspecies_meta.tsv\n",
            "data/influenza/h3_regions.txt\n",
            "data/influenza/h1_regions.txt\n",
            "data/influenza/fitness_doud2016/\n",
            "data/influenza/fitness_doud2016/Supplemental_File_5_sequencing_library_primers.txt\n",
            "data/influenza/fitness_doud2016/Supplemental_File_3_HApreferences_rescaled.txt\n",
            "data/influenza/fitness_doud2016/Doud2016_HA_replicate-2_prefs.csv\n",
            "data/influenza/fitness_doud2016/Doud2016_HA_replicate-3_prefs.csv\n",
            "data/influenza/fitness_doud2016/Doud2016_HA_replicate-1_prefs.csv\n",
            "data/influenza/fitness_doud2016/Supplemental_File_2_HApreferences.txt\n",
            "data/influenza/fitness_doud2016/Supplemental_File_6_WSN_to_H3_numbering_conversion.txt\n",
            "data/cov/\n",
            "data/cov/cov_all.fa\n",
            "data/cov/cov2_spike_wt.fasta\n",
            "data/cov/starr2020cov2/\n",
            "data/cov/starr2020cov2/binding_Kds.csv\n",
            "data/cov/starr2020cov2/expression_meanFs.csv\n",
            "data/cov/starr2020cov2/single_mut_effects.csv\n",
            "data/cov/gisaid.fasta\n",
            "data/cov/sarscov2_regions.txt\n",
            "data/cov/viprbrc_db.fasta\n",
            "data/cov/sars_cov2_seqs.fa\n",
            "data/cov/greaney2020cov2/\n",
            "data/cov/greaney2020cov2/escape_fracs.csv\n",
            "data/cov/greaney2020cov2/significant_escape_sites.csv\n",
            "data/target/\n",
            "data/target/flu/\n",
            "data/target/flu/clusters/\n",
            "data/target/flu/clusters/all_h1.fasta\n",
            "data/target/flu/clusters/all_h3.fasta\n",
            "data/target/flu/mutation/\n",
            "data/target/flu/mutation/mutations_h1.fa\n",
            "data/target/flu/mutation/mutations_h3.fa\n",
            "data/target/flu/evcouplings/\n",
            "data/target/flu/evcouplings/flu_h1/\n",
            "data/target/flu/evcouplings/flu_h1/mutate/\n",
            "data/target/flu/evcouplings/flu_h1/mutate/flu_h1_single_mutant_matrix.csv\n",
            "data/target/flu/evcouplings/flu_h3/\n",
            "data/target/flu/evcouplings/flu_h3/mutate/\n",
            "data/target/flu/evcouplings/flu_h3/mutate/flu_h3_single_mutant_matrix.csv\n",
            "data/target/flu/embedding/\n",
            "data/target/flu/embedding/tape_transformer_h1.npz\n",
            "data/target/flu/embedding/tape_transformer_h3.npz\n",
            "data/target/flu/embedding/unirep_h3.npz\n",
            "data/target/flu/embedding/unirep_h1.npz\n",
            "data/target/flu/embedding/bepler_ssa_h1.txt\n",
            "data/target/flu/embedding/bepler_ssa_h3.txt\n",
            "data/target/cov/\n",
            "data/target/cov/clusters/\n",
            "data/target/cov/clusters/all_sarscov2.fasta\n",
            "data/target/cov/mutation/\n",
            "data/target/cov/mutation/mutations_sarscov2.fa\n",
            "data/target/cov/evcouplings/\n",
            "data/target/cov/evcouplings/sarscov2/\n",
            "data/target/cov/evcouplings/sarscov2/mutate/\n",
            "data/target/cov/evcouplings/sarscov2/mutate/sarscov2_single_mutant_matrix.csv\n",
            "data/target/cov/embedding/\n",
            "data/target/cov/embedding/unirep_sarscov2.npz\n",
            "data/target/cov/embedding/tape_transformer_sarscov2.npz\n",
            "data/target/cov/embedding/bepler_ssa_sarscov2.txt\n",
            "data/target/hiv/\n",
            "data/target/hiv/clusters/\n",
            "data/target/hiv/clusters/all_BG505.fasta\n",
            "data/target/hiv/mutation/\n",
            "data/target/hiv/mutation/mutations_hiv.fa\n",
            "data/target/hiv/evcouplings/\n",
            "data/target/hiv/evcouplings/hiv_env/\n",
            "data/target/hiv/evcouplings/hiv_env/mutate/\n",
            "data/target/hiv/evcouplings/hiv_env/mutate/hiv_env_single_mutant_matrix.csv\n",
            "data/target/hiv/embedding/\n",
            "data/target/hiv/embedding/tape_transformer_hiv.npz\n",
            "data/target/hiv/embedding/unirep_hiv.npz\n",
            "data/target/hiv/embedding/bepler_ssa_hiv.txt\n"
          ]
        }
      ],
      "source": [
        "# unzip data\n",
        "!tar xvf data.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "! pip install anndata "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gLNHtsAsxJ",
        "outputId": "6a1114fa-a9cb-4086-81ba-c5f730730da2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anndata\n",
            "  Downloading anndata-0.7.8-py3-none-any.whl (91 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 41.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 40.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 71 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 91 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.19.5)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.7/dist-packages (from anndata) (21.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from anndata) (5.5.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from anndata) (3.1.0)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from anndata) (4.8.2)\n",
            "Requirement already satisfied: pandas>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.1.5)\n",
            "Requirement already satisfied: xlrd<2.0 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->anndata) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->anndata) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20->anndata) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.1->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.1->anndata) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.1->anndata) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->anndata) (1.5.2)\n",
            "Installing collected packages: anndata\n",
            "Successfully installed anndata-0.7.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scanpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU_3QNpWAzZP",
        "outputId": "69b7e35d-468d-4fa6-ada9-6e568a06bca5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Collecting sinfo\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.7.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.5)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.4.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.19.5)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: xlrd<2.0 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (3.0.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.0.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 67.6 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy) (2.7.3)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sinfo\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82709 sha256=9d46bb49c5206cc79de82efd96b4fa2487a4cc975df143f8cf4a79a675678859\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=108d041a692e58114f4f0d3845b2b2c27460f5da67d0c39f91701ce1d05681a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7899 sha256=8f5d0b39ae384fc94e3ec9b446d32e6d7852d106f172b32b27cb832587f85c2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ca/56/344d532fe53e855ccd6549795d370588ab8123907eecf4cf30\n",
            "Successfully built umap-learn pynndescent sinfo\n",
            "Installing collected packages: stdlib-list, pynndescent, umap-learn, sinfo, scanpy\n",
            "Successfully installed pynndescent-0.5.5 scanpy-1.8.2 sinfo-0.3.4 stdlib-list-0.8.0 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8zzlbc6A9gm",
        "outputId": "25cb73b7-e49f-494c-8741-b10d13fed736"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bio\n",
            "  Downloading bio-1.3.3-py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting biopython>=1.79\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bio) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bio) (2.23.0)\n",
            "Collecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->bio) (1.19.5)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (1.24.3)\n",
            "Installing collected packages: biothings-client, mygene, biopython, bio\n",
            "Successfully installed bio-1.3.3 biopython-1.79 biothings-client-0.2.6 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add mutation to path\n",
        "import sys\n",
        "sys.path.append('viral-mutation/bin')"
      ],
      "metadata": {
        "id": "SXi4OliI_iyL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m-OVQh1rRTIQ"
      },
      "outputs": [],
      "source": [
        "# from bin/flu.py\n",
        "from mutation import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## creating arguments dict\n",
        "\n",
        "\"\"\"\n",
        "def parse_args():\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Flu sequence analysis')\n",
        "    parser.add_argument('model_name', type=str,\n",
        "                        help='Type of language model (e.g., hmm, lstm)')\n",
        "    parser.add_argument('--namespace', type=str, default='flu',\n",
        "                        help='Model namespace')\n",
        "    parser.add_argument('--dim', type=int, default=512,\n",
        "                        help='Embedding dimension')\n",
        "    parser.add_argument('--batch-size', type=int, default=1000,\n",
        "                        help='Training minibatch size')\n",
        "    parser.add_argument('--n-epochs', type=int, default=14,\n",
        "                        help='Number of training epochs')\n",
        "    parser.add_argument('--seed', type=int, default=1,\n",
        "                        help='Random seed')\n",
        "    parser.add_argument('--checkpoint', type=str, default=None,\n",
        "                        help='Model checkpoint')\n",
        "    parser.add_argument('--train', action='store_true',\n",
        "                        help='Train model')\n",
        "    parser.add_argument('--train-split', action='store_true',\n",
        "                        help='Train model on portion of data')\n",
        "    parser.add_argument('--test', action='store_true',\n",
        "                        help='Test model')\n",
        "    parser.add_argument('--embed', action='store_true',\n",
        "                        help='Analyze embeddings')\n",
        "    parser.add_argument('--semantics', action='store_true',\n",
        "                        help='Analyze mutational semantic change')\n",
        "    parser.add_argument('--combfit', action='store_true',\n",
        "                        help='Analyze combinatorial fitness')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "arguments = namedtuple('arguments', ['model_name','train','test',\n",
        "                                     'dim','n_epochs','batch_size',\n",
        "                                     'namespace', 'seed','checkpoint','train_split'])\n",
        "\n",
        "args = arguments('bilstm', True, True,\n",
        "                 512,3,350, # defaults were batch-size 1000, 14 epochs\n",
        "                 'flu',1, None,None)\n",
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxGo51hQ_c7c",
        "outputId": "682d5907-0fea-4647-d1c8-1974b25cc575"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "arguments(model_name='bilstm', train=True, test=True, dim=512, n_epochs=3, batch_size=350, namespace='flu', seed=1, checkpoint=None, train_split=None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AAs = [\n",
        "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H',\n",
        "        'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W',\n",
        "        'Y', 'V', 'X', 'Z', 'J', 'U', 'B', 'Z'\n",
        "    ]\n",
        "    \n",
        "vocabulary = { aa: idx + 1 for idx, aa in enumerate(sorted(AAs)) }\n",
        "\n"
      ],
      "metadata": {
        "id": "rcWNMnJsCn7c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ5CI_R3Cn0Z",
        "outputId": "4816ac1b-3589-4441-b7d3-96c901355101"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': 1,\n",
              " 'B': 2,\n",
              " 'C': 3,\n",
              " 'D': 4,\n",
              " 'E': 5,\n",
              " 'F': 6,\n",
              " 'G': 7,\n",
              " 'H': 8,\n",
              " 'I': 9,\n",
              " 'J': 10,\n",
              " 'K': 11,\n",
              " 'L': 12,\n",
              " 'M': 13,\n",
              " 'N': 14,\n",
              " 'P': 15,\n",
              " 'Q': 16,\n",
              " 'R': 17,\n",
              " 'S': 18,\n",
              " 'T': 19,\n",
              " 'U': 20,\n",
              " 'V': 21,\n",
              " 'W': 22,\n",
              " 'X': 23,\n",
              " 'Y': 24,\n",
              " 'Z': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup(args):\n",
        "    fnames = [ 'data/influenza/ird_influenzaA_HA_allspecies.fa' ]\n",
        "    meta_fnames = [ 'data/influenza/ird_influenzaA_HA_allspecies_meta.tsv' ]\n",
        "\n",
        "    seqs = process(fnames, meta_fnames)\n",
        "\n",
        "    seq_len = max([ len(seq) for seq in seqs ]) + 2\n",
        "    vocab_size = len(AAs) + 2\n",
        "\n",
        "    model = get_model(args, seq_len, vocab_size)\n",
        "\n",
        "    return model, seqs"
      ],
      "metadata": {
        "id": "8iK1MMhfDLDF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(fnames, meta_fnames):\n",
        "    metas = load_meta(meta_fnames)\n",
        "\n",
        "    seqs = {}\n",
        "    for fname in fnames:\n",
        "        for record in SeqIO.parse(fname, 'fasta'):\n",
        "            if 'Reference_Perth2009_HA_coding_sequence' in record.description:\n",
        "                continue\n",
        "            if str(record.seq).count('X') > 10:\n",
        "                continue\n",
        "            if record.seq not in seqs:\n",
        "                seqs[record.seq] = []\n",
        "            accession = record.description.split('|')[0].split(':')[1]\n",
        "            seqs[record.seq].append(metas[accession])\n",
        "    return seqs"
      ],
      "metadata": {
        "id": "SKMId-YkDObw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_meta(meta_fnames):\n",
        "    metas = {}\n",
        "    for fname in meta_fnames:\n",
        "        with open(fname) as f:\n",
        "            header = f.readline().rstrip().split('\\t')\n",
        "            for line in f:\n",
        "                fields = line.rstrip().split('\\t')\n",
        "                accession = fields[1]\n",
        "                meta = {}\n",
        "                for key, value in zip(header, fields):\n",
        "                    if key == 'Subtype':\n",
        "                        meta[key] = value.strip('()').split('N')[0].split('/')[-1]\n",
        "                    elif key == 'Collection Date':\n",
        "                        meta[key] = int(value.split('/')[-1]) \\\n",
        "                                    if value != '-N/A-' else None\n",
        "                    elif key == 'Host Species':\n",
        "                        meta[key] = value.split(':')[1].split('/')[-1].lower()\n",
        "                    else:\n",
        "                        meta[key] = value\n",
        "                metas[accession] = meta\n",
        "    return metas"
      ],
      "metadata": {
        "id": "3m8ZVvqMDS7n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, seqs = setup(args)"
      ],
      "metadata": {
        "id": "OZ53TpRZCnvt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GoaH0EGCntY",
        "outputId": "ebdbc73d-39c9-461a-9882-98ec234d0672"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<language_model.BiLSTMLanguageModel at 0x7f685648ef90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1AOZq1eEwhA",
        "outputId": "d03a32b2-75c2-4c13-be18-cc88ded1f67f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44851"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seqs"
      ],
      "metadata": {
        "id": "50iLL1pVCnl5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "seqs_subset= defaultdict(dict)\n",
        "\n",
        "for x in list(seqs)[0:1000]:\n",
        "    seqs_subset[x] = seqs[x]\n"
      ],
      "metadata": {
        "id": "jdOOc2FcItPi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(seqs_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm1wess1J6Db",
        "outputId": "985da109-f814-4b3f-ede0-f683a0c6c45c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_seqs(seqs, split_method='random'):\n",
        "    train_seqs, test_seqs, val_seqs = {}, {}, {}\n",
        "\n",
        "    old_cutoff = 1990\n",
        "    new_cutoff = 2018\n",
        "\n",
        "    tprint('Splitting seqs...')\n",
        "    for seq in seqs:\n",
        "        # Pick validation set based on date.\n",
        "        seq_dates = [\n",
        "            meta['Collection Date'] for meta in seqs[seq]\n",
        "            if meta['Collection Date'] is not None\n",
        "        ]\n",
        "        if len(seq_dates) > 0:\n",
        "            oldest_date = sorted(seq_dates)[0]\n",
        "            if oldest_date < old_cutoff or oldest_date >= new_cutoff:\n",
        "                test_seqs[seq] = seqs[seq]\n",
        "                continue\n",
        "        train_seqs[seq] = seqs[seq]\n",
        "    tprint('{} train seqs, {} test seqs.'\n",
        "           .format(len(train_seqs), len(test_seqs)))\n",
        "\n",
        "    return train_seqs, test_seqs\n",
        "\n"
      ],
      "metadata": {
        "id": "PSK_BhOGE1K-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xj6WKp0kc3pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.train or args.train_split or args.test:\n",
        "        train_test(args, model, seqs_subset, vocabulary, split_seqs)\n"
      ],
      "metadata": {
        "id": "bznipDQ8E1N4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbd976f-6da4-406a-fb1a-0e4f9446b909"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1621/1621 [==============================] - 2294s 1s/step - loss: 2.9451 - accuracy: 0.0779\n",
            "Epoch 2/3\n",
            "1621/1621 [==============================] - 2291s 1s/step - loss: 1.2038 - accuracy: 0.6537\n",
            "Epoch 3/3\n",
            "1621/1621 [==============================] - 2288s 1s/step - loss: 0.1340 - accuracy: 0.9692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs, test_seqs = split_seqs(seqs_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkVt1bnwvIdb",
        "outputId": "627efb41-af45-4d5c-827a-5e5372658422"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-03 20:58:53.092750 | Splitting seqs...\n",
            "2022-01-03 20:58:53.120300 | 824 train seqs, 176 test seqs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saved model in target/flu/checkpoints/bilstm > saved to vsp project locally\n"
      ],
      "metadata": {
        "id": "_SmJ3AU8E1Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Influenza HA semantic embedding UMAPs and log files with statistics can be generated with the command"
      ],
      "metadata": {
        "id": "8zWxF4iK0FNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python viral-mutation/bin/flu.py bilstm --checkpoint target/flu/checkpoints/bilstm/bilstm_512-03.hdf5 --embed > flu_embed.log 2>&1"
      ],
      "metadata": {
        "id": "wIMibHYqE1Gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ef93f1-38df-47aa-afba-5e0887861687"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1:  2880 Killed                  python viral-mutation/bin/flu.py bilstm --checkpoint target/flu/checkpoints/bilstm/bilstm_512-03.hdf5 --embed > flu_embed.log 2>&1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-residue escape prediction using validation data from Doud et al. (2018) and Lee et al. (2019) can be done with the command"
      ],
      "metadata": {
        "id": "KQNd2_-35ikC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python viral-mutation/bin/flu.py bilstm --checkpoint target/flu/checkpoints/bilstm/bilstm_512-03.hdf5 --semantics \\\n",
        "    > flu_semantics.log 2>&1"
      ],
      "metadata": {
        "id": "qUiNt6NVE1D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60625b7e-e8da-408f-fb72-db57a595d90c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1:  2735 Killed                  python viral-mutation/bin/flu.py bilstm --checkpoint target/flu/checkpoints/bilstm/bilstm_512-03.hdf5 --semantics > flu_semantics.log 2>&1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combinatorial fitness experiments measuring correlation with grammaticality and semantic change using data from Doud and Bloom (2016) and from Wu et al. (2020) can be done with the command"
      ],
      "metadata": {
        "id": "GjKhhArO50vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python viral-mutation/bin/flu.py bilstm --checkpoint target/flu/checkpoints/bilstm/bilstm_512-03.hdf5 --combfit \\\n",
        "    > flu_combfit.log 2>&1"
      ],
      "metadata": {
        "id": "Cs47Dl9tE1Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf02fbb-93ab-4f53-af2d-5e8166adbad0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1:  2824 Killed                  python viral-mutation/bin/flu.py bilstm --checkpoint target/flu/checkpoints/bilstm/bilstm_512-03.hdf5 --combfit > flu_combfit.log 2>&1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tEpN6bbAE0-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LrdRi2pJKhm"
      },
      "source": [
        "## 1. The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6fWyfF4KQ1y"
      },
      "source": [
        "We obtained mutational fitness preference scores for HA H1 A/WSN/1933 (WSN33) mutants from [Doud and Bloom (17) ](https://www.mdpi.com/1999-4915/8/6/155)\n",
        "\n",
        "\n",
        "M. B. Doud, J. D. Bloom, Accurate measurement of the effects of all amino-acid mutations\n",
        "on influenza hemagglutinin. Viruses 8, 155 (2016). doi:10.3390/v8060155 Medline \n",
        "> enabling us to measure with greatly improved accuracy and reproducibility the effects of all amino-acid mutations to an H1 influenza hemagglutinin on viral replication in cell culture.\n",
        "\n",
        "> Our measurements confirm at much higher resolution the results of previous studies suggesting that antigenic sites on the globular head of hemagglutinin are highly tolerant of mutations. We also show that other regions of hemagglutinin—including the stalk epitopes targeted by broadly neutralizing antibodies—have a much lower inherent capacity to tolerate point mutations. The ability to accurately measure the effects of all influenza mutations should enhance efforts to understand and predict viral evolution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8jkGGpUKek5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "-fq2Ca8UL-bf",
        "outputId": "d0742cce-afb9-4aa5-9039-9ae35059c09c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f57df8a8afd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df_e = pd.read_csv)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "df_e = pd.read_csv(\"data/target/escape_results.txt\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpbK58CmMmqr"
      },
      "source": [
        "## Model predictions\n",
        "\n",
        "> For the CSCS, our empirical results seem consistently well-calibrated around β = 1 (equally weighting both terms), which we used in all of our experiments.\n",
        "\n",
        "> We use different aspects of the language model to describe semantic change and grammaticality. Semantic change is defined as the L1 norm of the between the average embedding across all positions of the original sequence and that of the mutated sequence, . The l 1 norm, chosen because of more favorable properties compared to other standard distance metrics Effectively, distances in embedding space approximate semantic change and the emitted probability approximates grammaticality.\n",
        "\n",
        "\n",
        "\n",
        "viral-mutation/results/flu/semantics/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NLRPcaOQQ5Co"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LYY6EH9rQOwR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AAkaSq4pMAqG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "946765c9-0262-4d6b-d6d6-31ad5898aaba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-cb2117dc60f3>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    > We obtained experimentally validated causal escape mutations to HA H1 WSN33 from\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "## Escape prediction validation\n",
        "\n",
        "> We obtained experimentally validated causal escape mutations to HA H1 WSN33 from\n",
        "Doud et al. (1), HA H3 Perth09 from Lee et al. (2)\n",
        "M. B. Doud, J. M. Lee, J. D. Bloom, How single mutations affect viral escape from broad and\n",
        "narrow antibodies to H1 influenza hemagglutinin. Nat. Commun. 9, 1386 (2018).\n",
        "doi:10.1038/s41467-018-03665-3 Medline\n",
        "J. M. Lee, R. Eguia, S. J. Zost, S. Choudhary, P. C. Wilson, T. Bedford, T. Stevens-Ayers, M.\n",
        "Boeckh, A. C. Hurt, S. S. Lakdawala, S. E. Hensley, J. D. Bloom, Mapping person-to-\n",
        "person variation in viral mutations that escape polyclonal serum targeting influenza\n",
        "hemagglutinin. eLife 8, e49324 (2019). doi:10.7554/eLife.49324 Medline\n",
        "We then made, in silico, all possible single-residue mutations to H1 WSN33, H3 Perth09…. for each of these mutations, we computed semantic change and grammaticality and combined these scores using the CSCS rank-based acquisition function as described in Note S2. For a given viral protein, the value of the CSCS acquisition function was used to rank all possible mutants.\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here, we map all single amino-acid mutations that increase resistance to broad antibodies to H1 hemagglutinin. Our approach not only identifies antigenic mutations but also quantifies their effect sizes. All antibodies select mutations, but the effect sizes vary widely. The virus can escape a broad antibody to hemagglutinin’s receptor-binding site the same way it escapes narrow strain-specific antibodies: via single mutations with huge effects. In contrast, broad antibodies to hemagglutinin’s stalk only select mutations with small effects. Therefore, among the antibodies we examine, breadth is an imperfect indicator of the potential for viral escape via single mutations. Antibodies targeting the H1 hemagglutinin stalk are quantifiably harder to escape than the other antibodies tested here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHwWXFJhO_wJ"
      },
      "source": [
        "## Escape prediction validation\n",
        "\n",
        "> We obtained experimentally validated causal escape mutations to HA H1 WSN33 from\n",
        "Doud et al. (1), HA H3 Perth09 from Lee et al. (2)\n",
        "\n",
        "> We then made, in silico, all possible single-residue mutations to H1 WSN33, H3 Perth09…. for each of these mutations, we computed semantic change and grammaticality and combined these scores using the CSCS rank-based acquisition function as described in Note S2. For a given viral protein, the value of the CSCS acquisition function was used to rank all possible mutants.\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "M. B. Doud, J. M. Lee, J. D. Bloom,[How single mutations affect viral escape from broad and\n",
        "narrow antibodies to H1 influenza hemagglutinin.](https://www.nature.com/articles/s41467-018-03665-3.pdf) Nat. Commun. 9, 1386 (2018).\n",
        "doi:10.1038/s41467-018-03665-3 Medline\n",
        "\n",
        "> Here, we map all single amino-acid mutations that increase resistance to broad antibodies to H1 hemagglutinin. Our approach not only identifies antigenic mutations but also quantifies their effect sizes. All antibodies select mutations, but the effect sizes vary widely. The virus can escape a broad antibody to hemagglutinin’s receptor-binding site the same way it escapes narrow strain-specific antibodies: via single mutations with huge effects. In contrast, broad antibodies to hemagglutinin’s stalk only select mutations with small effects. Therefore, among the antibodies we examine, breadth is an imperfect indicator of the potential for viral escape via single mutations. Antibodies targeting the H1 hemagglutinin stalk are quantifiably harder to escape than the other antibodies tested here.\n",
        "\n",
        "\n",
        "J. M. Lee, R. Eguia, S. J. Zost, S. Choudhary, P. C. Wilson, T. Bedford, T. Stevens-Ayers, M.\n",
        "Boeckh, A. C. Hurt, S. S. Lakdawala, S. E. Hensley, J. D. Bloom, [Mapping person-to-\n",
        "person variation in viral mutations that escape polyclonal serum targeting influenza\n",
        "hemagglutinin.](https://elifesciences.org/articles/49324.pdf) eLife 8, e49324 (2019). doi:10.7554/eLife.49324 Medline\n",
        "\n",
        "> Here, we map how all amino-acidmutations to influenza’s major surface protein affect viral neutralization by polyclonal human sera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZiVIfuePUnh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "viral-escape.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9LrdRi2pJKhm",
        "wpbK58CmMmqr",
        "nHwWXFJhO_wJ"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMVfAYckPXy5r7b4DlJGfea",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}