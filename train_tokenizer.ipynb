{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBUpiRbXaTwnd7b0wRT87D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/viral-escape/blob/main/train_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9PLwgo0TTTM",
        "outputId": "bafcbf13-0cb8-4fe1-f587-2123951a21bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/hiv/load_hiv_data.py /content"
      ],
      "metadata": {
        "id": "bFJ50cYtTU_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install anndata scanpy bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pJ_1EKuOVCYF",
        "outputId": "4028888a-abca-4d19-ef34-364b0a5663f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anndata\n",
            "  Downloading anndata-0.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting scanpy\n",
            "  Downloading scanpy-1.9.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 9.3 MB/s \n",
            "\u001b[?25hCollecting bio\n",
            "  Downloading bio-1.3.8-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.4.1)\n",
            "Requirement already satisfied: pandas>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.3.5)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from anndata) (5.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from anndata) (1.21.6)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.7/dist-packages (from anndata) (21.3)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.7/dist-packages (from anndata) (3.1.0)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from anndata) (4.11.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=3->anndata) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->anndata) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20->anndata) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.1->anndata) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.1->anndata) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.1->anndata) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.64.0)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Collecting matplotlib>=3.4\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Collecting session-info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (1.4.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
            "\u001b[K     |████████████████████████████████| 930 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bio) (2.23.0)\n",
            "Collecting biopython>=1.79\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 41.2 MB/s \n",
            "\u001b[?25hCollecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bio) (2021.10.8)\n",
            "Collecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: umap-learn, pynndescent, session-info\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=1140ff26b463a60dad8d4f04e9b6bec3cf347e229ee01341bf82e649dfe91036\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=c5eb7d12faacfeef7f337411d98cb80cf9f2500efdce70931687fef518501c97\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8048 sha256=509b54faeaa213b457ab007609226cced52f48f3405d2b18d0f71d8c5f98ba9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/ad/14/6a42359351a18337a8683854cfbba99dd782271f2d1767f87f\n",
            "Successfully built umap-learn pynndescent session-info\n",
            "Installing collected packages: fonttools, stdlib-list, pynndescent, matplotlib, biothings-client, umap-learn, session-info, mygene, biopython, anndata, scanpy, bio\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed anndata-0.8.0 bio-1.3.8 biopython-1.79 biothings-client-0.2.6 fonttools-4.33.3 matplotlib-3.5.2 mygene-3.2.2 pynndescent-0.5.7 scanpy-1.9.1 session-info-1.0.0 stdlib-list-0.8.0 umap-learn-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/viral-mutation/bin')"
      ],
      "metadata": {
        "id": "V0cIFcBlVLzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import load_hiv_data "
      ],
      "metadata": {
        "id": "haJJsPIxTYkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "importlib.reload(load_hiv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzdt78DPVv74",
        "outputId": "a62b3527-8f65-4d5f-eec6-272c84cbd5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'load_hiv_data' from '/content/load_hiv_data.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = load_hiv_data.load_hiv_data(n=1000)"
      ],
      "metadata": {
        "id": "qZm3YtO1VA1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHkqh7qFVRdI",
        "outputId": "56e00ced-4549-43b9-9ebc-106be625c17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Roberta "
      ],
      "metadata": {
        "id": "lG13mUZhW62R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install tokenizers"
      ],
      "metadata": {
        "id": "71qEOMOcWNNt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "outputId": "28ab48b2-68f4-4e17-e442-30de590da686"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_seqs(seqs, train_share=0.66):\n",
        "    train_seqs, test_seqs = {}, {}\n",
        "\n",
        "    print('Splitting seqs...')\n",
        "\n",
        "    import random \n",
        "    k = int(train_share*len(seqs))\n",
        "    train_idx = random.sample(range(len(seqs)),k)\n",
        "\n",
        "\n",
        "    for idx, seq in enumerate(seqs):\n",
        "        # Pick train set \n",
        "        if idx in train_idx:    \n",
        "            train_seqs[seq] = seqs[seq]\n",
        "        else:\n",
        "            test_seqs[seq] = seqs[seq]\n",
        "\n",
        "    print('{} train seqs, {} test seqs.'\n",
        "           .format(len(train_seqs), len(test_seqs)))\n",
        "\n",
        "    return train_seqs, test_seqs"
      ],
      "metadata": {
        "id": "muIVZAHzXgZX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.5*len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUL5-t9IY1sd",
        "outputId": "db1df225-423f-4f7b-a93e-d055fbcd5771"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500.0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs, test_seqs = split_seqs(seqs,0.66)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_T_PbPLYrBu",
        "outputId": "c2da56ec-e962-42bd-a249-dccaa6492a32"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting seqs...\n",
            "660 train seqs, 340 test seqs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make pretraining_data.txt file\n",
        "for x,seq in enumerate(list(train_seqs.keys())):\n",
        "    with open(\"drive/MyDrive/hiv/pretraining_data.txt\", 'a') as filehandle:\n",
        "        filehandle.write('%s\\n' % seq)\n",
        "\n",
        "# make eval_data.txt file\n",
        "for x,seq in enumerate(list(test_seqs.keys())):\n",
        "    with open(\"drive/MyDrive/hiv/eval_data.txt\", 'a') as filehandle:\n",
        "        filehandle.write('%s\\n' % seq)"
      ],
      "metadata": {
        "id": "TBu8Sae_Yw70"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "paths = ['drive/MyDrive/hiv/pretraining_data', 'drive/MyDrive/hiv/eval_data','drive/MyDrive/hiv/tokenizer']\n",
        "for path in paths:\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(path + \" created\")"
      ],
      "metadata": {
        "id": "xEe27BHrmmK-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_path = 'drive/MyDrive/hiv/pretraining_data/'\n",
        "\n",
        "for x,seq in enumerate(list(train_seqs.keys())):\n",
        "    with open(seq_path+str(x)+\".txt\", 'w') as filehandle:\n",
        "        filehandle.write('%s\\n' % seq)"
      ],
      "metadata": {
        "id": "eC7UR4XPk8VF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train tokenizer \n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())"
      ],
      "metadata": {
        "id": "Ky0SJfgmkHfq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AAs = [\n",
        "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H',\n",
        "        'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W',\n",
        "        'Y', 'V', 'X', 'Z', 'J', 'U', 'B',\n",
        "    ]\n",
        "vocabulary = { aa: idx + 1 for idx, aa in enumerate(sorted(AAs)) }"
      ],
      "metadata": {
        "id": "S2UiPSPOkkd_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper paramenter vocab_size\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(special_tokens=[\"<s>\", \n",
        "                                     \"</s>\",\n",
        "                                     \"<pad>\",\n",
        "                                \"<unk>\",\n",
        "                                \"<mask>\"],\n",
        "                     initial_alphabet=list(vocabulary.keys()),\n",
        "                     vocab_size=10_000)"
      ],
      "metadata": {
        "id": "fbXqE7WrkTpO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "paths = [str(x) for x in Path(\"drive/MyDrive/hiv/pretraining_data\").glob(\"**/*.txt\")]"
      ],
      "metadata": {
        "id": "PMhCHNttklVt"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_1l07CsktFo",
        "outputId": "20d52b30-9b9d-4cb4-e9a5-67defc80ff12"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/MyDrive/hiv/pretraining_data/0.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/1.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/2.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/3.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/4.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/5.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/6.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/7.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/8.txt',\n",
              " 'drive/MyDrive/hiv/pretraining_data/9.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_path = 'drive/MyDrive/hiv/eval_data/'\n",
        "\n",
        "for x,seq in enumerate(list(test_seqs.keys())):\n",
        "    with open(seq_path+str(x)+\".txt\", 'w') as filehandle:\n",
        "        filehandle.write('%s\\n' % seq)"
      ],
      "metadata": {
        "id": "HGpQfcTPkud_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(paths, trainer)"
      ],
      "metadata": {
        "id": "grV1RNfrlNYc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(single=\"<s> $A </s>\",\n",
        "                                              special_tokens=[\n",
        "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "SoFcye9LlUn7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tokenizer max_length\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "tokenizer.enable_padding(pad_token=\"<pad>\")"
      ],
      "metadata": {
        "id": "OUaSavIQlbe_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"drive/MyDrive/hiv/tokenizer/tokenizer.json\")\n",
        "tokenizer.model.save(\"drive/MyDrive/hiv/tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqMO-LoclpAz",
        "outputId": "e51d8be7-eb60-4b9f-ae22-bbbbc1b07ccf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/MyDrive/hiv/tokenizer/vocab.json',\n",
              " 'drive/MyDrive/hiv/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data_files(train_seqs, test_seqs):\n",
        "    # make train_data.txt file\n",
        "    for x,seq in enumerate(list(train_seqs.keys())):\n",
        "        with open(\"drive/MyDrive/hiv/pretraining_data.txt\", 'a') as filehandle:\n",
        "            filehandle.write('%s\\n' % seq)\n",
        "\n",
        "    # make eval_data.txt file\n",
        "    for x,seq in enumerate(list(test_seqs.keys())):\n",
        "        with open(\"drive/MyDrive/hiv/eval_data.txt\", 'a') as filehandle:\n",
        "            filehandle.write('%s\\n' % seq)\n",
        "\n",
        "    import os\n",
        "    # check paths if do not exists\n",
        "    paths = ['drive/MyDrive/hiv/pretraining_data', 'drive/MyDrive/hiv/eval_data']\n",
        "    for path in paths:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "            print(path + \" created\")\n",
        "\n",
        "    # prep training data directory for tokenizer\n",
        "    seq_path = 'drive/MyDrive/hiv/pretraining_data/'\n",
        "    for x,seq in enumerate(list(train_seqs.keys())):\n",
        "        with open(seq_path+str(x)+\".txt\", 'w') as filehandle:\n",
        "            filehandle.write('%s\\n' % seq)\n",
        "    print(\"training data files saved to \" + seq_path)\n",
        "    \n",
        "    # pret test data directory for tokenizer\n",
        "    seq_path = 'drive/MyDrive/hiv/eval_data/'\n",
        "    for x,seq in enumerate(list(test_seqs.keys())):\n",
        "        with open(seq_path+str(x)+\".txt\", 'w') as filehandle:\n",
        "            filehandle.write('%s\\n' % seq)\n",
        "    print(\"test data files saved to \" + seq_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "ngvFfikeltGI"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data_files(train_seqs, test_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kfuRfUZoVUa",
        "outputId": "82a5badd-5d48-4237-afe9-a470b17fc29b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data files saved to drive/MyDrive/hiv/pretraining_data/\n",
            "test data files saved to drive/MyDrive/hiv/eval_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(train_seqs, test_seqs, vocab_size=10_000, max_length=512):\n",
        "\n",
        "    # prep data files\n",
        "    prep_data_files(train_seqs, test_seqs)\n",
        "\n",
        "    # load tokenizer libraries  \n",
        "    from tokenizers import Tokenizer\n",
        "    from tokenizers.models import BPE\n",
        "\n",
        "    # initialize tokenizer\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    \n",
        "    # define vocabulary\n",
        "    AAs = [\n",
        "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H',\n",
        "        'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W',\n",
        "        'Y', 'V', 'X', 'Z', 'J', 'U', 'B',\n",
        "    ]\n",
        "    vocabulary = { aa: idx + 1 for idx, aa in enumerate(sorted(AAs)) }\n",
        "\n",
        "    # hyper paramenter vocab_size\n",
        "    from tokenizers.trainers import BpeTrainer\n",
        "    trainer = BpeTrainer(special_tokens=[\"<s>\", \n",
        "                                        \"</s>\",\n",
        "                                        \"<pad>\",\n",
        "                                    \"<unk>\",\n",
        "                                    \"<mask>\"],\n",
        "                        initial_alphabet=list(vocabulary.keys()),\n",
        "                        vocab_size=vocab_size)\n",
        "    \n",
        "    # directory for training data files\n",
        "    from pathlib import Path\n",
        "    paths = [str(x) for x in Path(\"drive/MyDrive/hiv/pretraining_data\").glob(\"**/*.txt\")]\n",
        "    tokenizer.train(paths, trainer)\n",
        "\n",
        "    # post processing\n",
        "    from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "    tokenizer.post_processor = TemplateProcessing(single=\"<s> $A </s>\",\n",
        "                                              special_tokens=[\n",
        "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "        ]\n",
        "    )\n",
        "    tokenizer.enable_truncation(max_length=max_length)\n",
        "    tokenizer.enable_padding(pad_token=\"<pad>\")\n",
        "    \n",
        "    import os\n",
        "    # check paths if do not exists\n",
        "    paths = ['drive/MyDrive/hiv/tokenizer', \"drive/MyDrive/hiv/tokenizer/tokenizer_{}maxlength_{}vocabsize\".format(max_length, vocab_size)]\n",
        "    for path in paths:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "            print(path + \" created\")\n",
        "\n",
        "\n",
        "    # save tokenizer model and dictionary\n",
        "    tokenizer.save(\"drive/MyDrive/hiv/tokenizer/tokenizer_{}maxlength_{}vocabsize.json\".format(max_length, vocab_size))\n",
        "    tokenizer.model.save(\"drive/MyDrive/hiv/tokenizer/tokenizer_{}maxlength_{}vocabsize\".format(max_length, vocab_size))\n",
        "    print(\"tokenizer model and dictionary with {} max_length and {} vocab_size saved\".format(max_length, vocab_size))"
      ],
      "metadata": {
        "id": "oQNN1NWQoYpV"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenizer(train_seqs, test_seqs, vocab_size=10_000, max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLphsXwfqiv3",
        "outputId": "ccbf627a-187a-4aef-a8b9-bf78ccdd6956"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data files saved to drive/MyDrive/hiv/pretraining_data/\n",
            "test data files saved to drive/MyDrive/hiv/eval_data/\n",
            "drive/MyDrive/hiv/tokenizer/tokenizer_512maxlength_10000vocabsize created\n",
            "tokenizer model and dictionary with 512 max_length and 10000 vocab_size saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rPmL6CVbrSGe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}